# Algorithm

**Big O Notation** is a mathematical notation used to describe the upper bound of an algorithm's runtime or space complexity in terms of the input size. It provides a high-level understanding of the algorithm's efficiency and performance in the worst-case scenario. Common Big O notations include:

* **O(1)**: Constant time complexity.
* **O(log n)**: Logarithmic time complexity.
* **O(n)**: Linear time complexity.
* **O(n log n)**: Linearithmic time complexity.
* **O(nÂ²)**: Quadratic time complexity.
* **O(2^n)**: Exponential time complexity.

Big O is crucial for evaluating how scalable an algorithm is as the input size grows.

<mark style="color:red;">The letter "N" typically represents the size of the input</mark> in Big O notation. It indicates the variable or parameter that scales with the problem's size or complexity.

<mark style="color:red;">The letter "O" stands for "Order Of." It signifies the upper bound or rate of growth of an algorithm's</mark> running time or space requirement in terms of the input size, N.

<figure><img src=".gitbook/assets/1_6mpaXFsrRPFXSKXK5Qgm8w.webp" alt=""><figcaption></figcaption></figure>
